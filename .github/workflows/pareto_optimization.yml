name: Pareto Optimization

on:
  schedule:
    # Run weekly on Sunday at 00:00 UTC
    - cron: '0 0 * * 0'
  workflow_dispatch:  # Manual trigger
    inputs:
      n_iterations:
        description: 'Number of optimization iterations'
        required: false
        default: '50'
      n_initial:
        description: 'Number of initial random samples'
        required: false
        default: '10'

jobs:
  pareto-optimization:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hours

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install torch botorch matplotlib scipy

      - name: Run Pareto optimization
        run: |
          python -c "
          from tfan.pareto_v2 import ParetoRunner, ParetoRunnerConfig

          config = ParetoRunnerConfig(
              n_initial_points=${{ github.event.inputs.n_initial || 10 }},
              n_iterations=${{ github.event.inputs.n_iterations || 50 }},
              output_dir='artifacts/pareto',
              seed=42,
          )

          runner = ParetoRunner(config)
          front = runner.run(verbose=True)

          print(f'\\nFinal Pareto front has {front.n_dominated} points')
          print(f'Hypervolume: {front.hypervolume:.2f}')
          "

      - name: Generate visualization report
        run: |
          python dashboards/pareto_app.py \
            --results artifacts/pareto/pareto_front.json \
            --output artifacts/pareto/report

      - name: Export best config
        run: |
          python -c "
          from tfan.pareto_v2 import ParetoRunner
          from pathlib import Path
          import yaml
          import time

          runner = ParetoRunner()
          runner.load_results('artifacts/pareto')

          # Get best config (prioritize accuracy, balance latency/energy)
          best = runner.get_best_config({
              'neg_accuracy': 10.0,   # maximize accuracy
              'latency': 1.0,
              'epr_cv': 2.0,
              'topo_gap': 1.0,
              'energy': 0.5,
          })

          # Export to configs/auto/
          out_dir = Path('configs/auto')
          out_dir.mkdir(parents=True, exist_ok=True)

          timestamp = int(time.time())
          best_path = out_dir / f'best_{timestamp}.yaml'
          latest_path = out_dir / 'best.yaml'

          with open(best_path, 'w') as f:
              yaml.safe_dump(best, f)
          with open(latest_path, 'w') as f:
              yaml.safe_dump(best, f)

          print(f'Exported best config to {best_path}')
          print(f'Updated {latest_path}')
          print(f'Config: {best}')
          "

      - name: Verify hard gates
        id: gates
        run: |
          python -c "
          import json
          from pathlib import Path

          results_path = Path('artifacts/pareto/pareto_front.json')
          baseline_path = Path('artifacts/pareto/baseline_hv.json')

          if results_path.exists():
              with open(results_path, 'r') as f:
                  results = json.load(f)

              n_pareto = results['n_pareto_points']
              hv = results['hypervolume']

              print(f'Pareto points: {n_pareto}')
              print(f'Hypervolume: {hv:.2f}')

              # Gate 1: >= 6 Pareto points
              assert n_pareto >= 6, f'GATE FAILED: Expected >= 6 Pareto points, got {n_pareto}'

              # Gate 2: HV >= baseline * 0.98 (allow 2% regression)
              if baseline_path.exists():
                  with open(baseline_path, 'r') as f:
                      baseline = json.load(f)
                  baseline_hv = baseline.get('hypervolume', 0)
                  threshold = baseline_hv * 0.98
                  assert hv >= threshold, f'GATE FAILED: HV {hv:.2f} < threshold {threshold:.2f}'
                  print(f'âœ“ HV {hv:.2f} >= baseline {baseline_hv:.2f} * 0.98')

              # Gate 3: Check best config metrics
              configs = results.get('configurations', [])
              if configs:
                  best_config = configs[0]  # First config after sorting
                  latency = best_config.get('latency_ms', 999)
                  epr_cv = best_config.get('epr_cv', 1.0)

                  assert latency <= 200, f'GATE FAILED: latency_p95 {latency}ms > 200ms'
                  assert epr_cv <= 0.15, f'GATE FAILED: EPR_CV {epr_cv:.3f} > 0.15'

                  print(f'âœ“ latency_p95: {latency}ms <= 200ms')
                  print(f'âœ“ EPR_CV: {epr_cv:.3f} <= 0.15')

              print('âœ“ All hard gates passed')

              # Save current HV as new baseline
              baseline_path.parent.mkdir(parents=True, exist_ok=True)
              with open(baseline_path, 'w') as f:
                  json.dump({'hypervolume': hv, 'timestamp': results.get('timestamp', '')}, f)

              # Set output for PR creation
              print('GATES_PASSED=true')
          else:
              raise FileNotFoundError('Results file not found')
          " | tee -a $GITHUB_OUTPUT

      - name: Upload Pareto front results
        uses: actions/upload-artifact@v3
        with:
          name: pareto-front-results
          path: |
            artifacts/pareto/pareto_front.json
            artifacts/pareto/pareto_objectives.npy
            artifacts/pareto/baseline_hv.json
            artifacts/pareto/report/
            configs/auto/
          retention-days: 30

      - name: Create PR for config promotion
        if: steps.gates.outputs.GATES_PASSED == 'true' && github.event_name == 'schedule'
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "chore(config): auto-promote Pareto-optimized config"
          branch: auto/promote-pareto-config
          delete-branch: true
          title: "ðŸŽ¯ Auto-promote Pareto-optimized configuration"
          body: |
            ## Automated Pareto Config Promotion

            This PR was automatically created by the weekly Pareto optimization workflow.

            **Gate Status:** âœ… All gates passed

            **Changes:**
            - Updated `configs/auto/best.yaml` with new Pareto-optimal configuration
            - See artifacts for detailed Pareto front analysis and visualizations

            **Next Steps:**
            1. Review the Pareto front results in workflow artifacts
            2. Validate the configuration meets requirements
            3. Merge to promote this config to production

            _Automated by `.github/workflows/pareto_optimization.yml`_
          labels: |
            automated
            config
            pareto

      - name: Create summary
        if: always()
        run: |
          echo "## Pareto Optimization Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f artifacts/pareto/pareto_front.json ]; then
            python -c "
            import json
            with open('artifacts/pareto/pareto_front.json', 'r') as f:
                results = json.load(f)

            print(f'**Pareto Points:** {results[\"n_pareto_points\"]}')
            print(f'**Hypervolume:** {results[\"hypervolume\"]:.2f}')
            print()
            print('**Top 3 Configurations:**')
            for i, config in enumerate(results['configurations'][:3]):
                print(f'{i+1}. n_heads={config[\"n_heads\"]}, d_model={config[\"d_model\"]}, keep_ratio={config[\"keep_ratio\"]:.2f}')
            " >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Optimization failed or results not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');

            if (fs.existsSync('artifacts/pareto/pareto_front.json')) {
              const results = JSON.parse(fs.readFileSync('artifacts/pareto/pareto_front.json', 'utf8'));

              const comment = `## ðŸŽ¯ Pareto Optimization Results

              **Pareto Points:** ${results.n_pareto_points}
              **Hypervolume:** ${results.hypervolume.toFixed(2)}

              **Gate Status:** ${results.n_pareto_points >= 6 ? 'âœ… PASSED' : 'âŒ FAILED'}

              See workflow artifacts for detailed results and visualizations.
              `;

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }

  pareto-comparison:
    runs-on: ubuntu-latest
    needs: pareto-optimization
    if: github.event_name == 'pull_request'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Checkout base branch
        run: |
          git fetch origin main
          git checkout main
          mkdir -p baseline_results

      - name: Run baseline optimization
        run: |
          python -c "
          from tfan.pareto_v2 import ParetoRunner, ParetoRunnerConfig

          config = ParetoRunnerConfig(
              n_initial_points=10,
              n_iterations=20,  # Shorter for baseline
              output_dir='baseline_results',
              seed=42,
          )

          runner = ParetoRunner(config)
          front = runner.run(verbose=False)
          " || echo "Baseline optimization failed (expected if new feature)"

      - name: Compare fronts
        run: |
          python -c "
          import json
          from pathlib import Path

          baseline_path = Path('baseline_results/pareto_front.json')
          current_path = Path('artifacts/pareto/pareto_front.json')

          if baseline_path.exists() and current_path.exists():
              with open(baseline_path, 'r') as f:
                  baseline = json.load(f)
              with open(current_path, 'r') as f:
                  current = json.load(f)

              hv_improvement = (current['hypervolume'] - baseline['hypervolume']) / baseline['hypervolume'] * 100

              print(f'Hypervolume improvement: {hv_improvement:.1f}%')

              if hv_improvement > 5:
                  print('âœ“ Significant improvement detected!')
          else:
              print('Baseline or current results not found. Skipping comparison.')
          "
