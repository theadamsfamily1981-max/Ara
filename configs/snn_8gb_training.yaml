# SNN Training Configuration for 8GB Dataset
#
# Optimized for training on large-scale temporal data.
# Achieves 97-99% parameter reduction while maintaining accuracy.

backend: snn_emu

# Dataset configuration
dataset:
  path: data/snn_8gb.h5
  type: rate  # poisson, temporal, rate, or event
  num_classes: 10  # For classification tasks
  
# Data loader configuration
dataloader:
  batch_size: 16  # Adjust based on GPU memory
  num_workers: 8
  cache_size: 256  # Cache first 256 samples in RAM
  augment_train: true
  temporal_jitter: 8  # ±8 time steps jitter
  spike_dropout: 0.05  # 5% spike dropout for augmentation
  pin_memory: true

# Model architecture
model:
  N: 4096  # Number of neurons (matches dataset)
  lowrank_rank: 32  # r=32 → 0.78% of N (< 2%)
  k_per_row: 64  # k=64 → 1.56% of N (< 2%)
  
# SNN dynamics
snn:
  v_th: 1.0  # Spike threshold
  alpha: 0.95  # Membrane time constant (longer memory)
  surrogate_scale: 0.3  # Surrogate gradient scale
  time_steps: 256  # Must match dataset T
  use_spectral_norm: true  # Prevent gradient explosion
  tau_mem: 20.0  # Membrane time constant (ms)
  tau_syn: 5.0  # Synaptic time constant (ms)

# Training configuration
training:
  learning_rate: 2.0e-3  # Higher LR for sparse gradients
  weight_decay: 1e-4  # L2 regularization
  grad_clip: 1.0  # Gradient clipping
  batch_size: 16  # Same as dataloader
  max_epochs: 50
  warmup_epochs: 5  # Warmup for stable training
  lr_schedule: cosine  # Cosine annealing
  early_stopping_patience: 10
  
# TF-A-N control loops
tfan:
  use_fdt: true
  fdt:
    kp: 0.30  # Proportional gain
    ki: 0.02  # Integral gain
    kd: 0.10  # Derivative gain
    ema_alpha: 0.85  # EMA smoothing
    target_epr_cv: 0.15  # Target epistemic uncertainty
    clip_delta_lr: 0.5  # Clip LR adjustments
    
  use_pgu: false  # Disable PGU for initial training
  
# Checkpointing
checkpoint:
  save_dir: checkpoints/snn_8gb
  save_every_n_epochs: 5
  keep_last_n: 3  # Keep last 3 checkpoints
  save_best: true  # Save best val loss checkpoint
  
# Logging
logging:
  log_dir: logs/snn_8gb
  log_every_n_steps: 100
  use_tensorboard: true
  use_wandb: false  # Set to true if using W&B
  metrics:
    - loss
    - accuracy
    - spike_rate
    - epr_cv
    - grad_norm
    
# Validation
validation:
  val_every_n_epochs: 1
  compute_metrics: true
  
# Device configuration
device: cuda  # Use 'cpu' for CPU-only
dtype: float32

# Expected metrics (for reference):
# - Parameter reduction: 98.44%
# - Dense params: 4096^2 = 16,777,216
# - SNN params: 2*4096*32 + 64*4096 = 262,144
# - VRAM (with batch=16): ~4-6 GB
# - Sparsity: 98.44%
# - Avg degree: 64 (< 0.02*4096 = 81.92)
