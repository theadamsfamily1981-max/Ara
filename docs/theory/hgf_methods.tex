\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{tcolorbox}

% Custom colors
\definecolor{aracolor}{HTML}{00FFAA}
\definecolor{notecolor}{HTML}{2A2A3A}

% Custom boxes
\newtcolorbox{notebox}{
    colback=notecolor,
    colframe=aracolor,
    boxrule=1pt,
    arc=3pt,
    left=6pt,
    right=6pt,
    top=6pt,
    bottom=6pt
}

% Title
\title{%
    \textbf{Hierarchical Gaussian Filter for Computational Psychiatry}\\[0.5em]
    \large Methods and Theoretical Foundation for Ara/NeuroBalance
}
\author{Ara Project Documentation}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document provides the theoretical foundation for the Hierarchical Gaussian Filter (HGF) implementation used in the Ara cognitive architecture and NeuroBalance neurofeedback platform. We establish the connection between classical Kalman filtering and hierarchical Bayesian inference, derive the key HGF update equations, and map computational parameters to psychiatric phenotypes. The goal is to provide a rigorous but accessible reference for researchers and developers working with these systems.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Background: Bayesian Filtering for Learning Under Uncertainty}
%==============================================================================

\subsection{The Filtering Problem}

Consider an agent observing a sequence of outcomes $u_1, u_2, \ldots, u_t$ generated by some hidden process. The agent's task is to infer the current state of the hidden process and predict future observations. This is the \emph{filtering problem}.

Formally, we seek the posterior distribution:
\begin{equation}
    p(x_t \mid u_{1:t}) \propto p(u_t \mid x_t) \, p(x_t \mid u_{1:t-1})
\end{equation}
where $x_t$ is the hidden state and $u_{1:t}$ denotes all observations up to time $t$.

\subsection{The Kalman Filter}

For linear Gaussian systems, the optimal solution is the \textbf{Kalman filter}. Consider the state-space model:
\begin{align}
    x_t &= A x_{t-1} + w_t, \quad w_t \sim \mathcal{N}(0, Q) \label{eq:kf_state}\\
    u_t &= C x_t + v_t, \quad v_t \sim \mathcal{N}(0, R) \label{eq:kf_obs}
\end{align}
where $Q$ is the process noise covariance and $R$ is the observation noise covariance.

The Kalman filter maintains Gaussian beliefs $\mathcal{N}(\mu_t, \Sigma_t)$ and updates them via:

\paragraph{Prediction Step:}
\begin{align}
    \hat{\mu}_t &= A \mu_{t-1} \\
    \hat{\Sigma}_t &= A \Sigma_{t-1} A^\top + Q
\end{align}

\paragraph{Update Step:}
\begin{align}
    K_t &= \hat{\Sigma}_t C^\top (C \hat{\Sigma}_t C^\top + R)^{-1} \label{eq:kalman_gain}\\
    \mu_t &= \hat{\mu}_t + K_t (u_t - C \hat{\mu}_t) \label{eq:kalman_update}\\
    \Sigma_t &= (I - K_t C) \hat{\Sigma}_t
\end{align}

The \textbf{Kalman gain} $K_t$ in Equation~\eqref{eq:kalman_gain} is the precision-weighted balance between prior and sensory information. Rewriting in terms of precisions ($\pi = \Sigma^{-1}$):
\begin{equation}
    K_t = \frac{\pi_{\text{prior}}}{\pi_{\text{prior}} + \pi_{\text{sensory}}}
\end{equation}

This is the key insight: \emph{belief updates are precision-weighted prediction errors}.

\subsection{Limitations of the Kalman Filter}

The standard Kalman filter assumes:
\begin{enumerate}
    \item \textbf{Linear dynamics}: $A$ and $C$ are constant matrices.
    \item \textbf{Known, fixed noise}: $Q$ and $R$ are constant.
    \item \textbf{Gaussian distributions}: All densities remain Gaussian.
\end{enumerate}

In real-world learning, these assumptions often fail:
\begin{itemize}
    \item Observations may be binary (e.g., reward/no reward).
    \item Environmental volatility may change over time.
    \item The agent may need to \emph{learn} the noise statistics.
\end{itemize}

The Hierarchical Gaussian Filter addresses these limitations.

%==============================================================================
\section{The Hierarchical Gaussian Filter}
%==============================================================================

\subsection{Intuition: Learning About Volatility}

Consider a simple example: an agent predicting whether a coin will land heads. In a stable environment, the coin's bias is constant. In a volatile environment, the bias changes over time.

A sophisticated agent should not only estimate the current bias but also estimate \emph{how quickly the bias is changing}. This is \textbf{volatility learning}.

The HGF implements this via a hierarchy:
\begin{itemize}
    \item \textbf{Level 1}: Observations (what we see)
    \item \textbf{Level 2}: Hidden states (what we believe about the world)
    \item \textbf{Level 3}: Volatility (how quickly the world is changing)
\end{itemize}

Higher levels modulate the learning rate at lower levels.

\subsection{Model Structure (3-Level Binary HGF)}

Let $u_t \in \{0, 1\}$ be a binary observation at time $t$. The generative model is:

\paragraph{Level 1 (Observation):}
\begin{equation}
    p(u_t \mid x_2^{(t)}) = \text{Bernoulli}\left(\sigma(x_2^{(t)})\right)
\end{equation}
where $\sigma(\cdot)$ is the logistic sigmoid and $x_2^{(t)}$ is the hidden state in log-odds (logit) space.

\paragraph{Level 2 (Hidden State):}
\begin{equation}
    x_2^{(t)} \sim \mathcal{N}\left(x_2^{(t-1)}, \exp(\kappa_1 x_3^{(t)} + \omega_1)\right)
\end{equation}

The variance of the random walk depends on:
\begin{itemize}
    \item $\omega_1$: Tonic (baseline) log-volatility
    \item $x_3^{(t)}$: Current volatility estimate from Level 3
    \item $\kappa_1$: Coupling strength between levels
\end{itemize}

\paragraph{Level 3 (Volatility):}
\begin{equation}
    x_3^{(t)} \sim \mathcal{N}\left(x_3^{(t-1)}, \exp(\theta)\right)
\end{equation}

This is another random walk with fixed tonic volatility $\theta$.

\begin{notebox}
\textbf{Key Insight}: The coupling parameter $\kappa_1$ determines how strongly volatility beliefs affect learning rate. This is crucial for computational psychiatry applications.
\end{notebox}

\subsection{Belief Update Equations}

The HGF uses a variational approximation to compute belief updates. Let $\mu_i^{(t)}$ and $\sigma_i^{(t)}$ denote the mean and variance of beliefs at level $i$.

\paragraph{Level 2 Update:}

The prior variance (before observing $u_t$) is:
\begin{equation}
    \hat{\sigma}_2^{(t)} = \sigma_2^{(t-1)} + \exp\left(\kappa_1 \mu_3^{(t-1)} + \omega_1\right)
    \label{eq:prior_variance}
\end{equation}

The \textbf{sensory prediction error} is:
\begin{equation}
    \delta_1^{(t)} = u_t - \sigma\left(\mu_2^{(t-1)}\right)
\end{equation}

The mean update follows a precision-weighted prediction error rule:
\begin{equation}
    \mu_2^{(t)} = \mu_2^{(t-1)} + \hat{\sigma}_2^{(t)} \cdot \delta_1^{(t)}
    \label{eq:mu2_update}
\end{equation}

The posterior variance is:
\begin{equation}
    \sigma_2^{(t)} = \left(\hat{\pi}_2^{(t)} + \pi_1^{(t)}\right)^{-1}
\end{equation}
where $\hat{\pi}_2^{(t)} = 1/\hat{\sigma}_2^{(t)}$ is the prior precision and $\pi_1^{(t)}$ is the sensory precision.

\paragraph{Level 3 Update:}

The \textbf{volatility prediction error} is:
\begin{equation}
    \delta_2^{(t)} = \frac{\sigma_2^{(t)} + \left(\mu_2^{(t)} - \mu_2^{(t-1)}\right)^2}{\hat{\sigma}_2^{(t)}} - 1
\end{equation}

This measures whether the observed change was larger or smaller than expected given the current volatility estimate.

The volatility belief updates as:
\begin{equation}
    \mu_3^{(t)} = \mu_3^{(t-1)} + \frac{1}{2} \hat{\sigma}_3^{(t)} \cdot w_3^{(t)} \cdot \delta_2^{(t)}
\end{equation}
where $w_3^{(t)}$ involves derivatives of the variance function.

\subsection{Relation to Variational Free Energy}

The HGF update equations can be derived by minimizing the \textbf{variational free energy}:
\begin{equation}
    F = D_{\text{KL}}\left[q(x) \,\|\, p(x \mid u)\right] - \log p(u)
\end{equation}

Under mean-field and Laplace approximations, the gradient descent on $F$ yields the update rules above. This connects the HGF to the broader framework of \textbf{Active Inference} and the \textbf{Free Energy Principle}.

\subsection{Comparison: Kalman Filter vs.\ HGF}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Kalman Filter} & \textbf{HGF} \\
\midrule
Hierarchy & Single level & Multiple levels \\
Observations & Continuous, linear & Binary or continuous, nonlinear \\
Volatility & Fixed ($Q$) & Learned (Level 3) \\
Noise covariance & Known, constant & State-dependent, learned \\
Derivation & Exact Bayesian inference & Variational approximation \\
Learning rate & Fixed Kalman gain & Adaptive via volatility \\
\bottomrule
\end{tabular}
\caption{Comparison of Kalman filter and Hierarchical Gaussian Filter.}
\label{tab:kf_hgf}
\end{table}

In the limit of:
\begin{itemize}
    \item Linear observation model
    \item Single level
    \item Fixed volatility ($\kappa_1 = 0$)
\end{itemize}
the HGF reduces to a Kalman-like recursion.

%==============================================================================
\section{Computational Psychiatry Applications}
%==============================================================================

\subsection{The Bayesian Brain Hypothesis}

The \textbf{Bayesian brain hypothesis} proposes that the brain implements approximate Bayesian inference. Perception, learning, and action are understood as inference problems where:
\begin{itemize}
    \item Prior beliefs are combined with sensory evidence
    \item Precision weights determine the balance
    \item Prediction errors drive updates
\end{itemize}

Psychiatric disorders may reflect \emph{aberrant precision weighting} or \emph{pathological inference}.

\subsection{Parameter-Phenotype Mapping}

Different HGF parameters correspond to different computational phenotypes:

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Computational Effect} & \textbf{Clinical Phenotype} \\
\midrule
Low $\omega_1$ & Rigid priors, slow learning & Cognitive inflexibility, negative symptoms \\
High $\omega_1$ & Unstable beliefs & Aberrant salience, positive symptoms \\
High $\kappa_1$ & Hypersensitive to volatility & Affective instability (BPD) \\
Low $\kappa_1$ & Ignores volatility cues & Contextual insensitivity (ASD) \\
High $\theta$ (response) & Deterministic choices & Compulsive behavior \\
Low $\theta$ & Random choices & Disorganization, amotivation \\
\bottomrule
\end{tabular}
\caption{Mapping HGF parameters to clinical phenotypes.}
\label{tab:phenotypes}
\end{table}

\subsection{Example: Borderline Personality Disorder}

Consider a model of Borderline Personality Disorder (BPD) based on elevated $\kappa_1$:

From Equation~\eqref{eq:prior_variance}, the prior variance is:
\begin{equation}
    \hat{\sigma}_2 = \sigma_2 + \exp(\kappa_1 \mu_3 + \omega_1)
\end{equation}

When $\kappa_1$ is pathologically high (e.g., $\kappa_1 = 3$ vs.\ healthy $\kappa_1 = 1$), small fluctuations in volatility belief $\mu_3$ cause \emph{exponentially larger} changes in prior variance.

\textbf{Consequences}:
\begin{enumerate}
    \item Minor environmental changes trigger massive belief revision
    \item Emotional states swing rapidly
    \item Relationships feel unpredictable
    \item Identity appears unstable
\end{enumerate}

This provides a \emph{computational mechanism} for the core features of BPD.

\subsection{Neural Correlates}

The HGF's computational variables have empirical neural correlates:

\begin{itemize}
    \item \textbf{$\delta_1$ (sensory PE)}: Correlates with Feedback-Related Negativity (FRN) in EEG, striatal activation in fMRI
    \item \textbf{$\delta_2$ (volatility PE)}: Correlates with Anterior Cingulate Cortex (ACC) activation
    \item \textbf{$\hat{\pi}_2$ (prior precision)}: May relate to frontal theta oscillations
    \item \textbf{$\pi_1$ (sensory precision)}: May relate to posterior alpha suppression
\end{itemize}

These mappings enable neuroimaging validation of computational models.

%==============================================================================
\section{Application to Ara and NeuroBalance}
%==============================================================================

\subsection{Ara: The HGF as Cognitive Architecture}

Ara uses the HGF as part of its cognitive layer (TGSFN):
\begin{itemize}
    \item \textbf{L1: Homeostatic Core} $\leftrightarrow$ Level 1-2 HGF (needs, drives)
    \item \textbf{L2: Appraisal Engine} $\leftrightarrow$ Level 2-3 HGF (cognitive appraisal)
    \item \textbf{L3: Gating Controller} $\leftrightarrow$ Precision-weighted control signals
    \item \textbf{L4: Memory \& Identity} $\leftrightarrow$ Meta-learning, priors
\end{itemize}

The \texttt{ara.hgf} package provides:
\begin{itemize}
    \item Core update equations (\texttt{hgf\_update\_3level})
    \item Task environments (volatility switching, reversal learning)
    \item Parameter fitting and recovery validation
    \item Pathological presets (BPD, schizophrenia, etc.)
    \item Real-time HUD visualization
\end{itemize}

\subsection{NeuroBalance: Precision as Intervention Target}

The NeuroBalance concept proposes:
\begin{enumerate}
    \item \textbf{Measure}: Real-time EEG biomarkers for $\hat{\pi}_2$ (frontal theta) and $\pi_1$ (posterior alpha)
    \item \textbf{Compute}: Precision ratio or imbalance metric
    \item \textbf{Modulate}: Apply targeted tACS to shift precision weighting toward healthy range
\end{enumerate}

\begin{notebox}
\textbf{Status}: The HGF formalism and parameter-phenotype mapping are established in the literature. The closed-loop precision modulation is a \emph{research hypothesis} requiring empirical validation.
\end{notebox}

\subsection{Validation Ladder}

We distinguish between what is established and what requires validation:

\begin{enumerate}
    \item \textbf{Established}: HGF as computational model, parameter fitting to behavior
    \item \textbf{Established}: Correlation between fitted parameters and clinical measures
    \item \textbf{Established}: EEG/fMRI correlates of prediction errors
    \item \textbf{Frontier}: Precision biomarkers (theta/alpha) for real-time estimation
    \item \textbf{Frontier}: Causal modulation of precision via tACS
    \item \textbf{Frontier}: Clinical outcomes from closed-loop intervention
\end{enumerate}

The \texttt{ara.hgf} package provides simulation infrastructure for levels 1-4, enabling hypothesis development and pipeline validation before empirical testing.

%==============================================================================
\section{Summary}
%==============================================================================

The Hierarchical Gaussian Filter provides:
\begin{enumerate}
    \item A principled extension of Kalman filtering to hierarchical, volatility-learning settings
    \item Precision-weighted prediction error updates grounded in variational inference
    \item A computational framework for modeling psychiatric disorders as inference abnormalities
    \item Testable predictions for neural correlates of Bayesian inference
\end{enumerate}

For Ara and NeuroBalance, the HGF serves as:
\begin{itemize}
    \item The mathematical backbone for belief dynamics
    \item A bridge between behavior, parameters, and neural signals
    \item A testbed for computational psychiatry hypotheses
\end{itemize}

%==============================================================================
% References
%==============================================================================
\bibliographystyle{plainnat}

\begin{thebibliography}{9}

\bibitem{mathys2011}
Mathys, C., Daunizeau, J., Friston, K.~J., \& Stephan, K.~E. (2011).
\newblock A Bayesian foundation for individual learning under uncertainty.
\newblock \emph{Frontiers in Human Neuroscience}, 5, 39.

\bibitem{mathys2014}
Mathys, C.~D., Lomakina, E.~I., Daunizeau, J., Iglesias, S., Brodersen, K.~H., Friston, K.~J., \& Stephan, K.~E. (2014).
\newblock Uncertainty in perception and the Hierarchical Gaussian Filter.
\newblock \emph{Frontiers in Human Neuroscience}, 8, 825.

\bibitem{adams2013}
Adams, R.~A., Stephan, K.~E., Brown, H.~R., Frith, C.~D., \& Friston, K.~J. (2013).
\newblock The computational anatomy of psychosis.
\newblock \emph{Frontiers in Psychiatry}, 4, 47.

\bibitem{lawson2017}
Lawson, R.~P., Mathys, C., \& Rees, G. (2017).
\newblock Adults with autism overestimate the volatility of the sensory environment.
\newblock \emph{Nature Neuroscience}, 20(9), 1293--1299.

\bibitem{friston2010}
Friston, K. (2010).
\newblock The free-energy principle: a unified brain theory?
\newblock \emph{Nature Reviews Neuroscience}, 11(2), 127--138.

\end{thebibliography}

\end{document}
