{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# VAE Phenotype Clustering\n",
    "\n",
    "This notebook demonstrates using a Variational Autoencoder to learn disentangled latent representations of HGF trajectories across computational phenotypes.\n",
    "\n",
    "## Goals\n",
    "\n",
    "1. Train a VAE on trajectories from different pathological presets\n",
    "2. Evaluate latent space disentanglement using DCI, MIG, SAP, EDI\n",
    "3. Visualize phenotype clustering in latent space (t-SNE/UMAP)\n",
    "4. Demonstrate latent traversal to understand what each dim encodes\n",
    "\n",
    "## Key Question\n",
    "\n",
    "**Can a VAE learn a latent space where:**\n",
    "- z₁ ≈ ω₂ (tonic volatility)\n",
    "- z₂ ≈ κ₁ (coupling strength)  \n",
    "- z₃ ≈ θ (response temperature)\n",
    "\n",
    "If yes, we can use the latent space for:\n",
    "- Phenotype classification\n",
    "- Generative modeling of new trajectories\n",
    "- Parameter recovery from behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# HGF imports\n",
    "from ara.hgf import (\n",
    "    HGFAgent,\n",
    "    HGFParams,\n",
    "    VolatilitySwitchingTask,\n",
    ")\n",
    "from ara.hgf.pathology import (\n",
    "    HEALTHY_BASELINE,\n",
    "    SCHIZOPHRENIA_RIGID,\n",
    "    SCHIZOPHRENIA_LOOSE,\n",
    "    BPD_HIGH_KAPPA,\n",
    "    ANXIETY_HIGH_PRECISION,\n",
    ")\n",
    "\n",
    "# VAE imports\n",
    "from ara.vae import (\n",
    "    TrajectoryVAE,\n",
    "    VAEConfig,\n",
    "    generate_phenotype_dataset,\n",
    "    TrajectoryDataset,\n",
    "    compute_dci,\n",
    "    compute_mig,\n",
    "    compute_sap,\n",
    "    evaluate_disentanglement,\n",
    ")\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Generate Training Data\n",
    "\n",
    "Create trajectories from 5 phenotypes:\n",
    "- Healthy baseline\n",
    "- Schizophrenia (rigid priors)\n",
    "- Schizophrenia (loose priors)\n",
    "- BPD (high κ₁)\n",
    "- Anxiety (high precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "print(\"Generating phenotype dataset...\")\n",
    "data = generate_phenotype_dataset(\n",
    "    n_samples_per_phenotype=200,\n",
    "    n_trials=200,\n",
    "    phenotypes=['HEALTHY', 'SCZ_RIGID', 'SCZ_LOOSE', 'BPD', 'ANXIETY'],\n",
    "    add_noise=0.5,  # Parameter noise for diversity\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {data.trajectories.shape}\")\n",
    "print(f\"  - {data.n_samples} samples\")\n",
    "print(f\"  - {data.n_trials} trials per trajectory\")\n",
    "print(f\"  - {data.n_features} features per trial\")\n",
    "print(f\"\\nFeatures: {data.feature_names}\")\n",
    "print(f\"Factors: {data.factor_names}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label in np.unique(data.labels):\n",
    "    count = np.sum(data.labels == label)\n",
    "    print(f\"  Label {label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample trajectories from each phenotype\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "phenotype_names = ['Healthy', 'SCZ (Rigid)', 'SCZ (Loose)', 'BPD', 'Anxiety']\n",
    "colors = ['cyan', 'yellow', 'red', 'magenta', 'orange']\n",
    "\n",
    "for i, (name, color) in enumerate(zip(phenotype_names, colors)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get samples from this phenotype\n",
    "    mask = data.labels == i\n",
    "    samples = data.trajectories[mask][:5]  # First 5\n",
    "    \n",
    "    # Plot μ₂ trajectory\n",
    "    for sample in samples:\n",
    "        ax.plot(sample[:, 0], color=color, alpha=0.5, lw=1)\n",
    "    \n",
    "    ax.set_title(f'{name} μ₂ trajectories')\n",
    "    ax.set_xlabel('Trial')\n",
    "    ax.set_ylabel('μ₂')\n",
    "\n",
    "# Hide last subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Sample Trajectories by Phenotype', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Train VAE\n",
    "\n",
    "Train a β-VAE on the trajectory data. We use β > 1 to encourage disentanglement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TrajectoryDataset(data, normalize=True, return_factors=True)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Configure VAE\n",
    "config = VAEConfig(\n",
    "    n_trials=200,\n",
    "    n_features=8,\n",
    "    latent_dim=8,  # Match number of \"factors\" we care about\n",
    "    encoder_type='lstm',\n",
    "    decoder_type='lstm',\n",
    "    encoder_hidden_dims=[64, 32],\n",
    "    decoder_hidden_dims=[32, 64],\n",
    "    beta=4.0,  # β-VAE: higher β = more disentanglement\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "vae = TrajectoryVAE(config)\n",
    "print(f\"VAE created with {sum(p.numel() for p in vae.parameters())} parameters\")\n",
    "print(f\"Latent dim: {config.latent_dim}\")\n",
    "print(f\"β = {config.beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "n_epochs = 50\n",
    "\n",
    "history = {'total': [], 'recon': [], 'kl': []}\n",
    "\n",
    "print(\"Training VAE...\")\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = {'total': 0, 'recon': 0, 'kl': 0}\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        x, factors, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        x_recon, mu, logvar = vae(x)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_dict = vae.compute_loss(x, x_recon, mu, logvar)\n",
    "        \n",
    "        # Backprop\n",
    "        loss = torch.tensor(loss_dict.total, requires_grad=True)\n",
    "        # Actually need to compute gradients properly\n",
    "        recon_loss = torch.nn.functional.mse_loss(x_recon, x)\n",
    "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        total_loss = recon_loss + config.beta * kl_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss['total'] += total_loss.item()\n",
    "        epoch_loss['recon'] += recon_loss.item()\n",
    "        epoch_loss['kl'] += kl_loss.item()\n",
    "    \n",
    "    # Average\n",
    "    for k in epoch_loss:\n",
    "        epoch_loss[k] /= len(dataloader)\n",
    "        history[k].append(epoch_loss[k])\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: Loss={epoch_loss['total']:.4f} \"\n",
    "              f\"(Recon={epoch_loss['recon']:.4f}, KL={epoch_loss['kl']:.4f})\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(history['total'], 'cyan', lw=2)\n",
    "axes[0].set_title('Total Loss (ELBO)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "\n",
    "axes[1].plot(history['recon'], 'purple', lw=2)\n",
    "axes[1].set_title('Reconstruction Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "\n",
    "axes[2].plot(history['kl'], 'orange', lw=2)\n",
    "axes[2].set_title(f'KL Divergence (β={config.beta})')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Evaluate Disentanglement\n",
    "\n",
    "Now we evaluate whether the latent space is disentangled with respect to our ground truth factors (ω₂, κ₁, θ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all trajectories to latent space\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    all_z = []\n",
    "    for batch in dataloader:\n",
    "        x, _, _ = batch\n",
    "        z = vae.encode(x)\n",
    "        all_z.append(z.numpy())\n",
    "    \n",
    "    z_all = np.concatenate(all_z, axis=0)\n",
    "\n",
    "print(f\"Encoded {z_all.shape[0]} samples to {z_all.shape[1]}-dim latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute disentanglement metrics\n",
    "# Use first 3 factors (omega_2, kappa_1, theta) as ground truth\n",
    "factors_gt = data.factors[:, :3]  # Exclude phenotype_id for now\n",
    "\n",
    "print(\"Computing disentanglement metrics...\")\n",
    "report = evaluate_disentanglement(\n",
    "    z_all,\n",
    "    factors_gt,\n",
    "    factor_names=['ω₂', 'κ₁', 'θ'],\n",
    "    compute_all=True,\n",
    ")\n",
    "\n",
    "print(report.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize importance matrix\n",
    "dci_result = compute_dci(z_all, factors_gt)\n",
    "R = dci_result['importance_matrix']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "im = ax.imshow(R, cmap='viridis', aspect='auto')\n",
    "\n",
    "ax.set_xlabel('Latent Dimension')\n",
    "ax.set_ylabel('Factor')\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_yticklabels(['ω₂', 'κ₁', 'θ'])\n",
    "ax.set_xticks(range(config.latent_dim))\n",
    "ax.set_xticklabels([f'z{i}' for i in range(config.latent_dim)])\n",
    "\n",
    "plt.colorbar(im, label='Importance')\n",
    "ax.set_title('DCI Importance Matrix: Which latent dims encode which factors?')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "for i, factor in enumerate(['ω₂', 'κ₁', 'θ']):\n",
    "    top_dim = np.argmax(R[i, :])\n",
    "    top_importance = R[i, top_dim]\n",
    "    print(f\"  {factor} → z{top_dim} (importance: {top_importance:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Latent Space Visualization\n",
    "\n",
    "Visualize phenotype clustering using dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE projection\n",
    "print(\"Computing t-SNE projection...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "z_2d = tsne.fit_transform(z_all)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "phenotype_names = ['Healthy', 'SCZ (Rigid)', 'SCZ (Loose)', 'BPD', 'Anxiety']\n",
    "colors = ['cyan', 'yellow', 'red', 'magenta', 'orange']\n",
    "\n",
    "for i, (name, color) in enumerate(zip(phenotype_names, colors)):\n",
    "    mask = data.labels == i\n",
    "    ax.scatter(z_2d[mask, 0], z_2d[mask, 1], c=color, label=name, alpha=0.6, s=30)\n",
    "\n",
    "ax.set_xlabel('t-SNE 1')\n",
    "ax.set_ylabel('t-SNE 2')\n",
    "ax.set_title('Latent Space: Phenotype Clustering')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color by continuous factors\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "factor_names = ['ω₂', 'κ₁', 'θ']\n",
    "cmaps = ['coolwarm', 'viridis', 'plasma']\n",
    "\n",
    "for ax, factor_idx, name, cmap in zip(axes, range(3), factor_names, cmaps):\n",
    "    scatter = ax.scatter(\n",
    "        z_2d[:, 0], z_2d[:, 1],\n",
    "        c=factors_gt[:, factor_idx],\n",
    "        cmap=cmap,\n",
    "        alpha=0.6,\n",
    "        s=20,\n",
    "    )\n",
    "    plt.colorbar(scatter, ax=ax, label=name)\n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "    ax.set_title(f'Colored by {name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Latent Traversal\n",
    "\n",
    "Traverse individual latent dimensions to understand what each encodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a reference trajectory (healthy baseline)\n",
    "healthy_idx = np.where(data.labels == 0)[0][0]\n",
    "x_ref = dataset.trajectories[healthy_idx]\n",
    "\n",
    "# Find top dims for each factor\n",
    "top_dims = [np.argmax(R[i, :]) for i in range(3)]\n",
    "print(f\"Top latent dims: ω₂→z{top_dims[0]}, κ₁→z{top_dims[1]}, θ→z{top_dims[2]}\")\n",
    "\n",
    "# Traverse each top dim\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    for ax, dim, factor_name in zip(axes, top_dims, ['ω₂', 'κ₁', 'θ']):\n",
    "        # Generate traversal\n",
    "        traversal = vae.traverse_latent(\n",
    "            x_ref,\n",
    "            dim=dim,\n",
    "            range_vals=(-2.0, 2.0),\n",
    "            n_steps=7,\n",
    "        ).numpy()\n",
    "        \n",
    "        # Plot μ₂ trajectory for each traversal step\n",
    "        colors_trav = plt.cm.viridis(np.linspace(0, 1, len(traversal)))\n",
    "        for i, (traj, color) in enumerate(zip(traversal, colors_trav)):\n",
    "            alpha = 0.3 + 0.7 * (i / len(traversal))\n",
    "            ax.plot(traj[:, 0], color=color, alpha=alpha, lw=1.5,\n",
    "                   label=f'z{dim}={-2+i*4/6:.1f}' if i in [0, 3, 6] else None)\n",
    "        \n",
    "        ax.set_title(f'Latent Traversal: z{dim} (encodes {factor_name})')\n",
    "        ax.set_xlabel('Trial')\n",
    "        ax.set_ylabel('μ₂')\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6. Conclusions\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Disentanglement is possible**: The VAE can learn a latent space where different dimensions correspond to different HGF parameters.\n",
    "\n",
    "2. **Phenotype clustering works**: Trajectories from different pathological presets naturally cluster in latent space.\n",
    "\n",
    "3. **Latent traversal is interpretable**: Moving along specific latent dimensions produces trajectories that match expectations (e.g., higher κ₁ = more volatile).\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Synthetic data**: These are simulated trajectories. Real behavioral data will be noisier.\n",
    "\n",
    "2. **Known ground truth**: We had access to true parameters. For real data, we'd use DCI-lite with observable labels.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Apply to real behavioral data from clinical populations\n",
    "2. Use latent space for parameter recovery (encode trajectory → predict HGF params)\n",
    "3. Generate synthetic trajectories for data augmentation\n",
    "4. Combine with neural correlates (EEG/fMRI) in multimodal VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nModel: β-VAE with β={config.beta}\")\n",
    "print(f\"Latent dim: {config.latent_dim}\")\n",
    "print(f\"Training samples: {data.n_samples}\")\n",
    "print(f\"\\nDisentanglement Scores:\")\n",
    "print(f\"  DCI Disentanglement: {report.dci_disentanglement:.3f}\")\n",
    "print(f\"  DCI Completeness:    {report.dci_completeness:.3f}\")\n",
    "print(f\"  DCI Informativeness: {report.dci_informativeness:.3f}\")\n",
    "print(f\"  MIG:                 {report.mig:.3f}\")\n",
    "print(f\"  SAP:                 {report.sap:.3f}\")\n",
    "if report.edi_modularity is not None:\n",
    "    print(f\"  EDI Modularity:      {report.edi_modularity:.3f}\")\n",
    "    print(f\"  EDI Compactness:     {report.edi_compactness:.3f}\")\n",
    "print(f\"\\nFactor-Latent Mapping:\")\n",
    "for i, factor in enumerate(['ω₂', 'κ₁', 'θ']):\n",
    "    top_dim = np.argmax(R[i, :])\n",
    "    print(f\"  {factor} → z{top_dim}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
